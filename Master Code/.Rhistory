#Reset row names again...not sure why I have to do this again
rownames(energy_clean_final) = NULL
#Since, I had to transform NM values, to NM. I need to rename New Mexico as "NM"
energy_clean_final[energy_clean_final == "NM."] = "NM"
#Remove columns with no observations
energy_clean_final = energy_clean_final[,-c(12)]
energy_clean_final = energy_clean_final[,-c(18:19)]
energy_clean_final = energy_clean_final[,-c(14)]
######### Clean Presidential Data ###########
#filter by year(2020)
president_1 = president |>
filter(year == 2020) |>
group_by(candidate)
#Check to see if all voting methods are the same, they aren't cannot delete
unique(president_1$mode)
#Delete unnecessary columns
president_1 = president_1[,-c(2) ]
president_1 = president_1[,-c(4:5) ]
president_1 = president_1[,-c(8) ]
#Filter out parties other than REP & DEM
president_2 = president_1 |>
filter(party == "REPUBLICAN" | party == "DEMOCRAT")
#Group data appropriately
president_3 = president_2 |>
group_by(state_po, candidate) |>
mutate(sum_state = sum(candidatevotes))
#Now we have a column with the total number of votes for each candidate in each state,
#and we can begin to clean the data of county information
president_4 = president_3[,-c(1) ]
president_4 = president_4[,-c(4) ]
president_4 = president_4[,-c(2) ]
president_4 = president_4[,-c(3:5) ]
#Now, we can keep only the unique rows in our dataest.
president_5 = distinct(president_4)
#Create dummy for who each state voted for
won = won = c(0,1,0,1,1,0,0,1,1,0,1,0,1,0,1,0,1,0,0,1,1,0,1,0,0,1,1,0,0,1,0,1,0,1,0,1,0,1,1,0,1,0,1,0,0,1,0,1,1,0,1,0,1,0,1,0,1,0,0,1,0,1,1,0,1,0,1,0,0,1,0,1,1,0,0,1,1,0,1,0,0,1,0,1,0,1,0,1,0,1,0,1,0,1,1,0,1,0,1,0,0,1)
#Append the datasets and remove columns that do not correspond to the winning candidate
president_6 = cbind(president_5, won)
president_7 = filter(president_6, ...4 == "1")
#Remove unnecessary columns
president_final = president_7[,-c(3:4)]#Remove DC from the observations
president_final = president_final[-c(9),]
#Presidential data is now clean and ready to merge
######### Governors Data ################
#Webscrape a table showing State Governors and their party
url = "https://en.wikipedia.org/wiki/List_of_current_United_States_governors"
page = read_html(url)
page
tables = page|> html_elements("table")
tables
governors = tables[[2]] |> html_table()
#Make table a dataframe
governors = as.data.frame(governors)
#Remove unnecessary columns
governors = governors[,-c(2,4,6:10)]
governors = governors[,-c(4)]
#Set row 1 as variable names,
colnames(governors) = governors[1,]
#Remove row 1
governors = governors[-c(1),]
#Remove column for name of governor
governors = governors[,-c(2)]
#Check variables for uniqueness
unique(governors$Party)
#We can see that there are two observations that don't make sense
governors[governors == "Republican[note 1]"] = "Republican"
governors[governors == "Democratic–Farmer–Labor"] = "Democratic"
unique(governors$Party)
#Create dummy variable for republican governor
governors = governors |>
mutate(rep = ifelse(Party == "Republican", 1, 0))
#Add our previously created fiftystates vector
governors_new = cbind(governors,fiftystates)
governors_final = governors_new[,-c(1:2)]
names(governors_final)[names(governors_final) == "fiftystates"] = "state"
#Change name of
governors_final[governors_final == "NM."] = "NM"
######### BIG DATASET WORK ############
#Make the  names of the Unique ID column, states, the same in each dataset
names(president_final)[names(president_final) == "state_po"] <- "state"
#Join all three datasets together using left join
joined = left_join(energy_clean_final, president_final, by = "state")
joined_2 = left_join(joined, governors_final, by = "state" )
#Change name of rep dummy variable in dataset
names(joined_2)[names(joined_2) == "rep"] <- "republican governor"
joined_2 = joined_2 |>
mutate(trump_vote = ifelse(candidate == "DONALD J TRUMP", 1, 0))
dataset_final = joined_2[,-c(18)]
#Rename variables in dataset to names easier to work with.
names(dataset_final)[names(dataset_final) == "Gross Domestic Product ($ billion)"] <- "GDP"
names(dataset_final)[names(dataset_final) == "Renewables (%)"] <- "Renewables"
names(dataset_final)[names(dataset_final) == "Motor Gasoline (Excludes Pipelines) (thousand barrels)"] <- "Motor Gas"
names(dataset_final)[names(dataset_final) == "Small-Scale Solar Photovoltaic Generation (thousand MWh)"] <- "Solar"
#Create a dummy for red and blue states respectively
dataset_final = dataset_final |>
mutate(red_red = ifelse(trump_vote == 1 & `republican governor` == 1, 1, 0))
dataset_final = dataset_final |>
mutate(blue_blue = ifelse(trump_vote == 0 & `republican governor` == 0, 1, 0))
pacman::p_load(tidyverse, tidymodels, skimr, glmnet, kknn)
data(ames)
# set seed and split
set.seed(062396)
ames_split   = initial_split(ames, prop = 0.8)
ames_training = training(ames_split)
ames_testing  = testing(ames_split)
recipe =
# Define the recipe: Rating predicted by all other vars in credit_train
recipe(Sale_Price ~ ., data = ames_training) |>
# Impute missing values (use means) for numeric predictors
step_impute_mean(all_numeric_predictors()) |>
# Impute missing values (by k-nearest neighbors) for categorical predictors
step_impute_knn(all_nominal_predictors(), neighbors = 5) |>
# Create polynomial terms for numeric predictors
step_poly(all_numeric_predictors(), degree=2) |>
# Create indicators for categorical predictors
step_dummy(all_nominal_predictors()) |>
# Remove predictors with near-zero variance (improves stability)
step_nzv(all_predictors())
# Create the cleaned dataset
houses_clean = recipe |> prep()
houses_clean <- houses_clean %>%
juice()
# Now I define a model and workflow
model_lm =
linear_reg() |>
set_engine("lm")
houses_cv = ames_training |> vfold_cv(v = 5)
tidy(houses_cv)
fit_lm_cv =
workflow() |>                 # Define a workflow
add_model(model_lm) |>        # Choose the model
add_recipe(recipe) |>  # Clean the data
fit_resamples(houses_cv)       # Estimate & cross-validate
# Check the performance
fit_lm_cv |> collect_metrics()
fit_lm_cv |> collect_metrics(summarize = F)
# tuning
# lasso regression
model_lasso =
linear_reg(penalty = tune(), mixture = 1) |>
set_engine("glmnet")
# instead of using fit_resample we use tune_grid()
lambdas = 10 ^ seq(from = 5, to = -2, length = 1e3)
workflow_lasso = workflow() |>
add_model(model_lasso) |>
add_recipe(recipe)
lasso_cv = workflow_lasso |>
tune_grid(
houses_cv,
grid = data.frame(penalty = lambdas),
metrics = metric_set(rmse)
)
lasso_cv |> show_best()
autoplot(lasso_cv, metric = "rmse")
# fit best model to testing data
final_lasso <- workflow_lasso %>%
finalize_workflow(select_best(lasso_cv, metric = "rmse"))
final_lasso
final_fit_lasso <- final_lasso %>%
last_fit(ames_split)
collect_metrics(lasso_cv)
autoplot(lasso_cv, metric = "rmse")
coefs = final_fit_lasso %>%
extract_fit_parsnip() %>%
tidy()
coefs_nonzero = coefs %>%
filter(estimate > 0)
coefs_nonzero
coefs = final_fit_lasso %>%
extract_fit_parsnip() %>%
tidy()
coefs_nonzero = coefs %>%
filter(estimate > 0)
coefs_nonzero
coefs = final_fit_lasso %>%
extract_fit_parsnip() %>%
tidy()
coefs_nonzero = coefs %>%
filter(estimate > 0)
coefs_nonzero
coefs = final_fit_lasso %>%
extract_fit_parsnip() %>%
tidy()
coefs_nonzero = coefs %>%
filter(estimate > 0)
coefs_nonzero
# Ranges for testing
lambdas = 10 ^ seq(from = 5, to = -2, length = 1e2)
alphas = seq(from = 0, to = 1, by = 0.1)
# Define the elasticnet model
model_net = linear_reg(penalty = tune(), mixture = tune()) %>%
set_engine("glmnet")
# Define the workflow
workflow_net = workflow() %>%
add_recipe(recipe) %>%
add_model(model_net)
# CV elasticnet with our ranges of lambdas and alphas
cv_net =
workflow_net %>%
tune_grid(
houses_cv,
grid = expand_grid(mixture = alphas, penalty = lambdas),
metrics = metric_set(rmse)
)
# choose model with smallest RMSE
final_net <- workflow_net %>%
finalize_workflow(select_best(cv_net, metric = "rmse"))
final_fit_net = final_net %>%
last_fit(ames_split)
final_fit_net %>% collect_metrics()
autoplot(lasso_cv, metric = "rmse")
pacman::p_load(tidyverse, tidymodels, skimr, glmnet, kknn)
data(ames)
# set seed and split
set.seed(12345)
data_split   = initial_split(ames, prop = 0.8)
houses_train = training(data_split)
houses_test  = testing(data_split)
recipe =
# Define the recipe: Rating predicted by all other vars in credit_train
recipe(Sale_Price ~ ., data = houses_train) |>
# Impute missing values (use means) for numeric predictors
step_impute_mean(all_numeric_predictors()) |>
# Impute missing values (by k-nearest neighbors) for categorical predictors
step_impute_knn(all_nominal_predictors(), neighbors = 5) |>
# Create polynomial terms for numeric predictors
step_poly(all_numeric_predictors(), degree=2) |>
# Create indicators for categorical predictors
step_dummy(all_nominal_predictors()) |>
# Remove predictors with near-zero variance (improves stability)
step_nzv(all_predictors())
# Create the cleaned dataset
houses_clean = recipe |> prep()
houses_clean <- houses_clean %>%
juice()
# define a model
# workflow
model_lm =
linear_reg() |>
set_engine("lm")
houses_cv = houses_train |> vfold_cv(v = 5)
tidy(houses_cv)
fit_lm_cv =
workflow() |>                 # Define a workflow
add_model(model_lm) |>        # Choose the model
add_recipe(recipe) |>  # Clean the data
fit_resamples(houses_cv)       # Estimate & cross-validate
# Check the performance
fit_lm_cv |> collect_metrics()
fit_lm_cv |> collect_metrics(summarize = F)
## tuning
# lasso regression
model_lasso =
linear_reg(penalty = tune(), mixture = 1) |>
set_engine("glmnet")
# instead of using fit_resample we use tune_grid()
lambdas = 10 ^ seq(from = 5, to = -2, length = 1e3)
workflow_lasso = workflow() |>
add_model(model_lasso) |>
add_recipe(recipe)
lasso_cv = workflow_lasso |>
tune_grid(
houses_cv,
grid = data.frame(penalty = lambdas),
metrics = metric_set(rmse)
)
lasso_cv |> show_best()
autoplot(lasso_cv, metric = "rmse")
# fit best model to testing data
final_lasso <- workflow_lasso %>%
finalize_workflow(select_best(lasso_cv, metric = "rmse"))
final_lasso
final_fit_lasso <- final_lasso %>%
last_fit(data_split)
collect_metrics(lasso_cv)
autoplot(lasso_cv, metric = "rmse")
coefs = final_fit_lasso %>%
extract_fit_parsnip() %>%
tidy()
coefs_nonzero = coefs %>%
filter(estimate > 0)
coefs_nonzero
# Ranges for testing
lambdas = 10 ^ seq(from = 5, to = -2, length = 1e2)
alphas = seq(from = 0, to = 1, by = 0.1)
# Define the elasticnet model
model_net = linear_reg(penalty = tune(), mixture = tune()) %>%
set_engine("glmnet")
# Define the workflow
workflow_net = workflow() %>%
add_recipe(recipe) %>%
add_model(model_net)
# CV elasticnet with our ranges of lambdas and alphas
cv_net =
workflow_net %>%
tune_grid(
houses_cv,
grid = expand_grid(mixture = alphas, penalty = lambdas),
metrics = metric_set(rmse)
)
# choose model with smallest RMSE
final_net <- workflow_net %>%
finalize_workflow(select_best(cv_net, metric = "rmse"))
final_fit_net = final_net %>%
last_fit(data_split)
final_fit_net %>% collect_metrics()
# Ranges for testing
lambdas = 10 ^ seq(from = 5, to = -2, length = 1e2)
alphas = seq(from = 0, to = 1, by = 0.1)
# Define the elasticnet model
model_net = linear_reg(penalty = tune(), mixture = tune()) %>%
set_engine("glmnet")
# Define the workflow
workflow_net = workflow() %>%
add_recipe(recipe) %>%
add_model(model_net)
# CV elasticnet with our ranges of lambdas and alphas
cv_net =
workflow_net %>%
tune_grid(
houses_cv,
grid = expand_grid(mixture = alphas, penalty = lambdas),
metrics = metric_set(rmse)
)
# choose model with smallest RMSE
final_net <- workflow_net %>%
finalize_workflow(select_best(cv_net, metric = "rmse"))
final_fit_net = final_net %>%
last_fit(ames_split)
final_fit_net %>% collect_metrics()
final_net
if (!require("pacman")) install.packages("pacman")
pacman::p_load(tidyverse, DBI, dbplyr, RSQLite, bigrquery, nycflights13, tictoc)
litecon = dbConnect(RSQLite::SQLite(), path = ":memory:")
copy_to(
litecon = connection,
df = nycflights13::flights,
name = "flights",
temporary = FALSE,
indexes = list(c("year", "month", "day"), "carrier", "tailnum", "dest"))
copy_to(
dest = litecon,
df = nycflights13::flights,
name = "flights",
temporary = FALSE,
indexes = list(c("year", "month", "day"), "carrier", "tailnum", "dest"))
flights_db = tbl(litecon, "flights")
class(flights_db)
flights_db
flights_db |> select(year:day, dep_delay, arr_delay)
flights_db |>
group_by(dest) |>
summarize(delay = mean(dep_time))
tic()
tailnum_delay_tv = flights |>
group_by(tailnum) |>
summarize(
mean_dep_delay = mean(dep_delay),
mean_arr_delay = mean(arr_delay),
n = n()
) %>%
filter(n > 100) |>
arrange(desc(mean_arr_delay))
toc()
tic()
tailnum_delay_db = flights_db |>
group_by(tailnum) |>
summarize(
mean_dep_delay = mean(dep_delay),
mean_arr_delay = mean(arr_delay),
n = n()
) %>%
filter(n > 100) |>
arrange(desc(mean_arr_delay))
toc()
tic()
print(tailnum_delay_db, n = 4)
toc()
tailnum_delay = tailnum_delay_db |> collect()
class(tailnum_delay)
tailnum_delay
tailnum_delay |> ggplot(aes(x = mean_dep_delay, y = mean_arr_delay)) +
geom_point(alpha = 0.3) +
geom_abline(intercept = 0, slope = 1, col="orange")
tailnum_delay_db = flights_db |>
group_by(tailnum) |>
summarize(
mean_dep_delay = mean(dep_delay),
mean_arr_delay = mean(arr_delay),
n = n()
) %>%
filter(n > 100) |>
arrange(desc(mean_arr_delay))
tailnum_delay_db |> show_query()
flights_db |>
filter(dep_delay > 240) |>
head(5) |>
show_query()
dbGetQuery(connection, "SELECT * FROM flights WHERE dep_delay > 240.0 LIMIT 5")
dbListTables(connection)
dbListFields(connection, "flights")
flights_db |>
select(distance, air_time) |>
mutate(speed = distance / (air_time / 60))
dbGetQuery(connection, "SELECT * FROM flights WHERE dep_delay > 240.0 LIMIT 5")
dbListTables(connection)
dbListFields(connection, "flights")
copy_to(
dest = lite_con,
df = nycflights13::weather,
name = "weather",
temporary = FALSE,
indexes = list(c("year", "month", "day", "hour"))
)
dbListTables(lite_con)
copy_to(
dest = lite_con,
df = nycflights13::weather,
name = "weather",
temporary = FALSE,
indexes = list(c("year", "month", "day", "hour"))
)
dbListTables(lite_con)
dbGetQuery(lite_con, "SELECT * FROM flights WHERE dep_delay > 240.0 LIMIT 5")
dbGetQuery(litecon, "SELECT * FROM flights WHERE dep_delay > 240.0 LIMIT 5")
dbListTables(litecon)
dbListFields(litecon, "flights")
SELECT *
FROM flights
WHERE dep_delay > 240
LIMIT 5
copy_to(
dest = lite_con,
df = nycflights13::weather,
name = "weather",
temporary = FALSE,
indexes = list(c("year", "month", "day", "hour"))
)
copy_to(
dest = litecon,
df = nycflights13::weather,
name = "weather",
temporary = FALSE,
indexes = list(c("year", "month", "day", "hour"))
)
dbGetQuery(litecon, "SELECT * FROM flights WHERE dep_delay > 240.0 LIMIT 5")
dbListTables(litecon)
dbGetQuery(litecon, "SELECT * FROM flights WHERE dest > 240.0 LIMIT 5")
SELECT *
FROM flights
copy_to(
dest = litecon,
df = nycflights13::weather,
name = "weather",
temporary = FALSE,
indexes = list(c("year", "month", "day", "hour"))
)
copy_to(
dest = litecon,
df = nycflights13::weather,
name = "weather",
temporary = FALSE,
indexes = list(c("year", "month", "day", "hour"))
)
copy_to(
dest = lite_con,
df = nycflights13::weather,
name = "weather",
temporary = FALSE,
indexes = list(c("year", "month", "day", "hour"))
)
copy_to(
dest = litecon,
df = nycflights13::weather,
name = "weather",
temporary = FALSE,
indexes = list(c("year", "month", "day", "hour"))
)
copy_to(
dest = lite_con,
df = nycflights13::weather,
name = "weather",
temporary = FALSE,
indexes = list(c("year", "month", "day", "hour"))
)
# Ranges for testing
lambdas = 10 ^ seq(from = 5, to = -2, length = 1e2)
alphas = seq(from = 0, to = 1, by = 0.1)
# Define the elasticnet model
model_net = linear_reg(penalty = tune(), mixture = tune()) %>%
set_engine("glmnet")
# Define the workflow
workflow_net = workflow() %>%
add_recipe(recipe) %>%
add_model(model_net)
# CV elasticnet with our ranges of lambdas and alphas
cv_net =
workflow_net %>%
tune_grid(
houses_cv,
grid = expand_grid(mixture = alphas, penalty = lambdas),
metrics = metric_set(rmse)
)
# choose model with smallest RMSE
final_net <- workflow_net %>%
finalize_workflow(select_best(cv_net, metric = "rmse"))
final_fit_net = final_net %>%
last_fit(ames_split)
final_fit_net %>% collect_metrics()
final_net
dbDisconnect(lite_con)
dbDisconnect(litecon)
